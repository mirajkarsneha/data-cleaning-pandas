{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "591197b0-4675-4d15-a0dd-6d77e22cc102",
   "metadata": {},
   "outputs": [],
   "source": [
    "## five data cleaning techniques\n",
    "# 1.Identify missing values, outliers, and inconsistencies.\n",
    "# 2.Clean missing data, remove duplicates, standardize text.\n",
    "# 3.Handle outliers and convert data types.\n",
    "# 4.Create new features, normalize data, and encode categorical variables.\n",
    "# 5.Save the cleaned and transformed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0df3a5ee-1889-4e7a-9cc4-86073a4b4dfe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     17\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnverified\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuestionable\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnder investigation\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnconfirmed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39mnan)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m############################################  Activity Column  ####################################################\u001b[39;00m\n\u001b[1;32m     23\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActivity\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActivity\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from functions import clean_dates, clean_year_column, replace_with_average, extract_numbers, clean_age, clean_gender_column, remove_female_and_nan\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#########################################  Load and inspect raw data file ###########################################\n",
    "\n",
    "##Read excel file\n",
    "url = 'https://www.sharkattackfile.net/spreadsheets/GSAF5.xls'\n",
    "df = pd.read_excel(url)\n",
    "\n",
    "\n",
    "df['Date'] = df['Date'].apply(clean_dates)\n",
    "\n",
    "df['Year'] = df['Year'].apply(clean_year_column)\n",
    "\n",
    "############################################  Type Column  #########################################################\n",
    "df[\"Type\"] = df[\"Type\"].str.strip()\n",
    "df[\"Type\"] = df[\"Type\"].replace(['Unverified', 'Questionable', 'Under investigation'], 'Unconfirmed')\n",
    "df[\"Type\"] = df[\"Type\"].replace('?', np.nan)\n",
    "\n",
    "\n",
    "############################################  Activity Column  ####################################################\n",
    "\n",
    "df['Activity'] = df['Activity'].astype(str).replace('nan', '')\n",
    "df['Activity'] = df['Activity'].str.replace(r'^\\d+$', '', regex=True)\n",
    "\n",
    "\n",
    "############################################  Name Column  ####################################################\n",
    "df['Name'] = df['Name'].apply(remove_female_and_nan)\n",
    "\n",
    "############################################  Sex Column  #########################################################\n",
    "df['Sex'] = df['Sex'].apply(clean_gender_column)\n",
    "\n",
    "############################################  Age Column  #########################################################\n",
    "\n",
    "#Replace two numbers which has & with average result of two numbers\n",
    "df['Age'] = df['Age'].apply(replace_with_average)\n",
    "\n",
    "#Find all numbers (including decimals) in the string\n",
    "df['Age'] = df['Age'].apply(extract_numbers).astype(float).round().astype('Int64')\n",
    "\n",
    "#remove Â½ and keep only numbers\n",
    "df['Age'] = df['Age'].apply(clean_age).astype(float).round().astype('Int64')\n",
    "\n",
    "#Replace <NA> with empty values\n",
    "df['Age'] = df['Age'].astype(str).replace('<NA>', '')\n",
    "\n",
    "############################################  Injury Column  #########################################################\n",
    "df[\"Injury\"] = df[\"Injury\"].str.strip()\n",
    "# rename to fatal if fatal is in string\n",
    "df[\"Injury\"] = df[\"Injury\"].apply(lambda x: \"Fatal\" if pd.notna(x) and \"FATAL\" in x.upper() and \"NOT FATAL\" not in x.upper() else x)\n",
    "\n",
    "# rename non Fatal if \"Fatal\" not in string\n",
    "df[\"Injury\"] = df[\"Injury\"].apply(lambda x: \"Non-Fatal\" if x != \"Fatal\" else x) \n",
    "\n",
    "############################################  Species Column  #########################################################\n",
    "df = df.rename(columns={'Species ': 'Species'})\n",
    "df = df.rename(columns={'original order': 'Original Order'})\n",
    "\n",
    "############################################  Time Column  #########################################################\n",
    "time_list = list(df[\"Time\"].unique())\n",
    "pattern = r'\\d{2}h\\d{2}'\n",
    "\n",
    "time_list = [item for item in time_list if isinstance(item, str) and re.search(pattern, item)]\n",
    "\n",
    "time_list\n",
    "\n",
    "replaced_list = []\n",
    "\n",
    "for time in time_list:\n",
    "    if \"-\" in time:\n",
    "        replaced_list.append(time)\n",
    "\n",
    "replaced_list\n",
    "\n",
    "df[\"Time\"] = df[\"Time\"].replace({'-16h30':'16h30',\n",
    " '09h00-10h00':'09h30',\n",
    " '14h00-15h00':'14h30',\n",
    " '14h00  -15h00':'14h30',\n",
    " '10h45-11h15':'11h00',\n",
    " '07h00 - 08h00':'07h30',\n",
    " '18h15-18h30':'18h30',\n",
    " '09h00 - 09h30':'09:15',\n",
    " '10h00 -- 11h00':'10h30',\n",
    " '09h00 -10h00':'09h30',\n",
    " '14h00 - 15h00':'14h40',\n",
    " '03h45 - 04h00':'04h00',\n",
    " '11h01 -time of ship sinking':'11h00',\n",
    " 'Ship aban-doned at 03h10':'03h00',\n",
    " '06h00 -- 07h00':'06h30',\n",
    " '17h00-18h00':'17h30',\n",
    " '19h00-20h00':'19h30'})\n",
    "\n",
    "df[\"Time\"].unique()\n",
    "\n",
    "replaced_list_1 = []\n",
    "\n",
    "for time in df[\"Time\"]:\n",
    "    # Check if the 'time' is not NaN and does not match the pattern using re.search()\n",
    "    if pd.notna(time) and not re.search(pattern, str(time)):  # Check for NaN and regex match\n",
    "        replaced_list_1.append(time)\n",
    "\n",
    "len(replaced_list_1)\n",
    "set(replaced_list_1)\n",
    "\n",
    "df[\"Time\"] = df[\"Time\"].replace({\n",
    " '\"After dark\"':'21h30',\n",
    " '\"After lunch\"':'13h00',\n",
    " '\"Early evening\"':'18h00',\n",
    " '\"Evening\"':'20h00',\n",
    " '\"Midday\"':'12h00',\n",
    " '\"Night\"':'22h00',\n",
    " '\"shortly before dusk\"':'06h00',\n",
    " 'After Dusk':'09h00',\n",
    " 'After dusk':'09h00',\n",
    " 'After midnight':'01h00',\n",
    " 'After noon':'13h00',\n",
    " 'Afternoon':'15h00',\n",
    " 'Before daybreak':'06h00',\n",
    " 'Dark':'22h00',\n",
    " 'Dawn':'20h00',\n",
    " 'Daybreak':'08h00',\n",
    " 'Daytime':'12h00',\n",
    " 'Dusk':'08:00',\n",
    " 'Early  morning':'07h00',\n",
    " 'Early Morning':'07h00',\n",
    " 'Early afternoon':'14h00',\n",
    " 'Early morning':'07h00',\n",
    " 'Evening':'20h00',\n",
    " 'Just before dawn':'20h00',\n",
    " 'Just before noon':'11h00',\n",
    " 'Just before sundown':'19h00',\n",
    " 'Late Afternoon':'17h00',\n",
    " 'Late afternon':'17h00',\n",
    " 'Late afternoon':'17h00',\n",
    " 'Late morning':'10h00',\n",
    " 'Late night':'23h00',\n",
    " 'Lunchtime':'12h00',\n",
    " 'Mid afternoon':'15h00',\n",
    " 'Mid morning':'15h00',\n",
    " 'Mid-morning':'15h00',\n",
    " 'Midday':'12h00',\n",
    " 'Midday.':'12h00',\n",
    " 'Midnight':'24h00',\n",
    " 'Morning':'08:00',\n",
    " 'Morning ':'08:00',\n",
    " 'Night':'22:00',\n",
    " 'Nightfall':'21:00',\n",
    " 'Noon':'12h00',\n",
    " 'Shortly after midnight':'01:00',\n",
    " 'Sunset':'20h00',\n",
    " 'dusk':'08h00',\n",
    " 'night':'22h00','01:00':'01h00',\n",
    " '0500':'05h00',\n",
    " '06j00':'22h00',\n",
    " '0830':'08h30',\n",
    " '08:00':'08h00',\n",
    " '09:15':'09h15',\n",
    " '10j30':'10h30',\n",
    " '10jh45':'10h45',\n",
    " '11hoo':'11h00',\n",
    " 1300:'13h00',\n",
    " 1415:'14h15',\n",
    " 1500:'15h00',\n",
    " '15j45':'15h45',\n",
    " '1600':'16h00',\n",
    " '20:00':'20h00',\n",
    " '21:00':'21h00',\n",
    " '22:00':'22h00',\n",
    " '8:04 pm':'20h00'})\n",
    "\n",
    "df[\"Time\"].unique()\n",
    "\n",
    "for time in df[\"Time\"]:\n",
    "    # Check if the 'time' is not NaN and does not match the pattern using re.search()\n",
    "    if pd.notna(time) and not re.search(pattern, str(time)):  # Check for NaN and regex match\n",
    "        replaced_list_1.append(time)\n",
    "\n",
    "set(replaced_list_1)\n",
    "\n",
    "pattern = r'\\d{2}h\\d{2}'\n",
    "\n",
    "matched_items = [item for item in time_list if re.search(pattern, item)]\n",
    "\n",
    "# make all remaining data formats in 'Time' that dont match the HHhMM format nan:\n",
    "df['Time'] = df['Time'].where(df['Time'].str.match(r'^\\d{2}h\\d{2}$'), np.nan)\n",
    "df['Time']\n",
    "df['Time'].isna().sum()\n",
    "\n",
    "#################################### Hypothesis 1 ######################################\n",
    "# Analysis and Plotting for \n",
    "# Sharks are more likely to attack when provoked than they are unprovoked\n",
    "df_replaced = df.replace(['Watercraft', 'Invalid', 'Unconfirmed', 'Sea Disaster', 'Boat'], np.nan)\n",
    "df_cleaned = df_replaced.dropna()\n",
    "grouped = df_replaced.groupby(['Type','Injury'])['Activity'].count()\n",
    "grouped\n",
    "\n",
    "###################################### Hypothesis 2 ################################################\n",
    "# Analysis and Plotting for H2\n",
    "# The frequency of shark attacks is higher during the early morning (dawn) and late afternoon (dusk) compared to midday\n",
    "\n",
    "# create new DF for H2\n",
    "H2_df = df.copy()\n",
    "\n",
    "# change time format for plotting\n",
    "H2_df['Time'].str.replace('h', ':', regex=False)\n",
    "\n",
    "# dropping all rows that are NaN in the time column\n",
    "H2_df = H2_df.dropna(subset = 'Time')\n",
    "\n",
    "# create hour column for plotting (we're ignoring minutes for the plot)\n",
    "H2_df['Hour'] = H2_df['Time'].str[:2]\n",
    "print(H2_df.shape)\n",
    "\n",
    "# groupby hour and then get the nuber of rows\n",
    "attack_counts = H2_df.groupby('Hour').size()\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(attack_counts.index, attack_counts.values, linestyle='-', color='r')\n",
    "plt.title('Number of shark attacks throughout the day')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Number of shark attacks')\n",
    "plt.grid(True)\n",
    "plt.savefig('attacks_throughout_day.png')\n",
    "plt.show()\n",
    "\n",
    "############################################## Hypothesis 3 ##########################################\n",
    "# Analysis and Plotting for H4\n",
    "# Surfing results in a higher number of shark attacks compared to other water-based activities\n",
    "# create new DF for H4\n",
    "H4_df = df.copy()\n",
    "# dropping all rows that are NaN in the time column\n",
    "H4_df.dropna(subset = 'Activity')\n",
    "# groupby activity\n",
    "activity_counts = H4_df.groupby('Activity').size()\n",
    "# only take those with more than 5 occurences, only take the top 5\n",
    "filtered_activity_counts = activity_counts[activity_counts > 5].sort_values(ascending = False)[:5]\n",
    "print(filtered_activity_counts)\n",
    "\n",
    "#Save cleaned data file\n",
    "df.to_csv('/Users/rishikeshdhokare/Documents/Ironhack/MiniProject/Quest2-Shark-Attacks/data-cleaning-pandas/sharkattacks_cleaned_data.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9019bc90-2f11-478d-a30c-c32b97070c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
